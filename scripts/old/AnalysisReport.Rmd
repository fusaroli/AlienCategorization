---
title: "Report on the Alien Analysis"
author: "Riccardo Fusaroli"
date: "11 Feb 2016"
output: html_document
---

- Analysis 1: training phase performance by condition and complexity (session)
- Analysis 2: test phase performance by condition and complexity (session)
- Analysis 3: learning curve analyses

TO DO

- Analysis 4: Controlling for (self-reported) Motivation, Effort, Attention (SURVEY) / RT (TRANSCRIPT)
- Analysis 5: Looking at the role of Complementarity (survey) / Cosine (transcript)
- Analysis 6: Looking at the role of common ground: Reciprocal Understanding (Survey) / Equal contribution (Transcript)


# Define whether assumptions are tested (slow)
```{r Assumptions}
AssumptionTesting=F
```

# Load libraries and data
# Define correct data types

```{r LibrariesData, echo=F, warning=F, message=F}
# Load libraries and data
library(pacman)
p_load(lme4,MuMIn,ggplot2,influence.ME,boot,plyr,entropy,BayesFactor,broom,pastecs,tidyr)

setwd('/Users/semrf/Dropbox/MyGitHub/AlienCategorization')
# Load the performance data and ensure the data structure is right

Data=read.table('AlienData.txt', sep=",", header=TRUE, stringsAsFactors = F)
Data$condition=factor(Data$condition, levels=c("1", "2"), labels=c("Dyads", "Individuals"))
Data$subject[Data$condition=='Dyads']=Data$subject[Data$condition=='Dyads']+30
Data$subject=as.factor(Data$subject)
Data$session=as.numeric(Data$session)
Data$cycle=as.numeric(Data$cycle)
Data$cycle[Data$cycle==0]=4
Data$trial=as.numeric(Data$trial)
Data$test=as.factor(Data$test)
Data$stimulus=as.factor(Data$stimulus)
Data$category=as.factor(Data$category)
Data$response=as.factor(Data$response)
Data$dangerous=as.factor(Data$dangerous)
Data$nutricious=as.factor(Data$nutricious)
Data$correct=as.factor(Data$correct)
Data$cumulative=as.numeric(Data$cumulative)
Data$RT=as.numeric(Data$RT)
Data$condition=relevel(Data$condition, "Individuals")

DataSurvey=read.table('AlienSurvey.txt', header=TRUE) # Name of the file

DataSurvey$Subject=factor(DataSurvey$Subject)
DataSurvey$Condition=factor(DataSurvey$Condition)

```

# Load a couple of additional functions (SEs, bootstrap and diagnostics)

```{r Functions, echo=F, warning=F, message=F}

# Loading a couple of functions to calculate standard errors
# The functions have been taken from the R cookbook: http://www.cookbook-r.com/Graphs/Plotting_means_and_error_bars_(ggplot2)/

summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE,
                      conf.interval=.95, .drop=TRUE) {
        require(plyr)
        
        # New version of length which can handle NA's: if na.rm==T, don't count them
        length2 <- function (x, na.rm=FALSE) {
                if (na.rm) sum(!is.na(x))
                else       length(x)
                }
        
        # This does the summary. For each group's data frame, return a vector with
        # N, mean, and sd
        datac <- ddply(data, groupvars, .drop=.drop,
                       .fun = function(xx, col) {
                               c(N    = length2(xx[[col]], na.rm=na.rm),
                                 mean = mean   (xx[[col]], na.rm=na.rm),
                                 sd   = sd     (xx[[col]], na.rm=na.rm)
                                 )
                               },
                       measurevar
                       )
        
        # Rename the "mean" column    
        datac <- rename(datac, c("mean" = measurevar))
        
        datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean
        
        # Confidence interval multiplier for standard error
        # Calculate t-statistic for confidence interval: 
        # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
        ciMult <- qt(conf.interval/2 + .5, datac$N-1)
        datac$ci <- datac$se * ciMult
        
        return(datac)
        }

summarySEwithin <- function(data=NULL, measurevar, betweenvars=NULL, withinvars=NULL,
                            idvar=NULL, na.rm=FALSE, conf.interval=.90, .drop=TRUE) {
        
        # Ensure that the betweenvars and withinvars are factors
        factorvars <- vapply(data[, c(betweenvars, withinvars), drop=FALSE],
                             FUN=is.factor, FUN.VALUE=logical(1))
        
        if (!all(factorvars)) {
                nonfactorvars <- names(factorvars)[!factorvars]
                message("Automatically converting the following non-factors to factors: ",
                        paste(nonfactorvars, collapse = ", "))
                data[nonfactorvars] <- lapply(data[nonfactorvars], factor)
                }
        
        # Get the means from the un-normed data
        datac <- summarySE(data, measurevar, groupvars=c(betweenvars, withinvars),
                           na.rm=na.rm, conf.interval=conf.interval, .drop=.drop)
        
        # Drop all the unused columns (these will be calculated with normed data)
        datac$sd <- NULL
        datac$se <- NULL
        datac$ci <- NULL
        
        # Norm each subject's data
        ndata <- normDataWithin(data, idvar, measurevar, betweenvars, na.rm, .drop=.drop)
        
        # This is the name of the new column
        measurevar_n <- paste(measurevar, "_norm", sep="")
        
        # Collapse the normed data - now we can treat between and within vars the same
        ndatac <- summarySE(ndata, measurevar_n, groupvars=c(betweenvars, withinvars),
                            na.rm=na.rm, conf.interval=conf.interval, .drop=.drop)
        
        # Apply correction from Morey (2008) to the standard error and confidence interval
        #  Get the product of the number of conditions of within-S variables
        nWithinGroups    <- prod(vapply(ndatac[,withinvars, drop=FALSE], FUN=nlevels,
                                        FUN.VALUE=numeric(1)))
        correctionFactor <- sqrt( nWithinGroups / (nWithinGroups-1) )
        
        # Apply the correction factor
        ndatac$sd <- ndatac$sd * correctionFactor
        ndatac$se <- ndatac$se * correctionFactor
        ndatac$ci <- ndatac$ci * correctionFactor
        
        # Combine the un-normed means with the normed results
        merge(datac, ndatac)
        }

normDataWithin <- function(data=NULL, idvar, measurevar, betweenvars=NULL,
                           na.rm=FALSE, .drop=TRUE) {
        require(plyr)
        
        # Measure var on left, idvar + between vars on right of formula.
        data.subjMean <- ddply(data, c(idvar, betweenvars), .drop=.drop,
                               .fun = function(xx, col, na.rm) {
                                       c(subjMean = mean(xx[,col], na.rm=na.rm))
                                       },
                               measurevar,
                               na.rm
                               )
        
        # Put the subject means with original data
        data <- merge(data, data.subjMean)
        
        # Get the normalized data in a new column
        measureNormedVar <- paste(measurevar, "_norm", sep="")
        data[,measureNormedVar] <- data[,measurevar] - data[,"subjMean"] +
                mean(data[,measurevar], na.rm=na.rm)
        
        # Remove this subject mean column
        data$subjMean <- NULL
        
        return(data)
}

# Loading a function to calculate and report relevant properties of the statistical models

ModelDiagnostics <- function(model,linear=F){
  # Calculating the p-value of the single factors
  coefs = data.frame(coef(summary(model))) 
  if (linear==T){
    coefs$p = format.pval(2*(1-pnorm(abs(coefs$t.value))), digits=2, eps=0.0001) 
  }
  # Calculating the R2s of the model
  r2=r.squaredGLMM(model)
  
  if (AssumptionTesting==T){
    #Testing for influential observations
    alt.est <- influence(model, group="Child.ID")
    SigChange <- sigtest(alt.est, test=-1.96)$condition
    if ('TRUE' %in% SigChange$Changed.Sig){
      cat("WARNING! The model is not robust!")
      match('TRUE',SigChange$Changed.Sig)
    } else { cat('All good the model is robust')}
    
    plot(model)
    qqnorm(resid(model))
    hist(resid(model))
  }
  
  Preds=row.names(coefs)
  
  txt=sprintf("R2m = %.2f, R2 = %.2f \n", r2[1], r2[2])
  cat(txt)
  
  for (i in c(2:length(coefs[,1]))){
    if (linear==T){
    txt=sprintf("%s: β = %.2f, SE = %.2f, t-stat = %.2f, p = %s \n", Preds[i], coefs[i,1], coefs[i,2], coefs[i,4], coefs[i,5])}
    else{
      txt=sprintf("%s: β = %.2f, SE = %.2f, t-stat = %.2f, p = %s \n", Preds[i], coefs[i,1], coefs[i,2], coefs[i,3], coefs[i,4])}
    cat(txt)
  }
}

# Loading a function to bootstrap means
boot.mean<-function(data,d)
{  return(mean(data[d],na.rm=T)) }

```

### Analysis 1 - Controlling for differences in RT, effort and motivation by condition

```{r SurveyTest}

modelRT=lmer(RT~1+condition+session+trial+(1+session+trial|subject)+(1+condition+session+trial|stimulus),REML=F,Data)
ModelDiagnostics(modelRT,linear=T)

modelMotivation=lmer(Motivation~1+Condition+(1|Pair),REML=F,DataSurvey)
ModelDiagnostics(modelMotivation,linear=T)

#by(Data$Motivation,Data$Condition,stat.desc)

modelEffort=lmer(Effort~1+Condition+(1|Pair),REML=F,DataSurvey)
ModelDiagnostics(modelEffort,linear=T)

#by(Data$Effort,Data$Condition,stat.desc)


```

### Analysis 2 - Performance (correctness) as a function of condition (individual vs dyad) and complexity (session)

N.B. We are excluding the post-test performance (rule-testing)

We use a mixed logistic regression model to assess main effects of condition (pair vs. individual) and complexity (session 1 to 3). We remove individual variability (random intercept plus random slopes due to session) and stimulus variability (random intercepts).

### The first model tests main effects

```{r TrainingMainEffects, echo=F, warning=F, message=F}

model1 <- glmer(correct~1+condition + session + (1+session|subject)+(1+condition+session|stimulus),family="binomial", Data[Data$test==0,],nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))

ModelDiagnostics(model1)


model1trial <- glmer(correct~1+condition + session + trial + (1+session+trial|subject)+(1+condition+session+trial|stimulus),family="binomial", Data[Data$test==0,],nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))

ModelDiagnostics(model1trial)

model1RT <- glmer(correct~1+condition+session+RT+trial+(1+session|subject)+(1+condition+session|stimulus),family="binomial", Data[Data$test==0,],nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))
ModelDiagnostics(model1RT)

```

### The second model tests interaction effects

```{r TrainingInteractions, echo=F, warning=F, message=F}

model2 <- glmer(correct~1+condition*(session)+(1+session|subject)+(1+condition+session|stimulus), family="binomial",Data[Data$test==0,],nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))
p=anova(model1,model2)
p
ModelDiagnostics(model2)


model2trial <- glmer(correct~1+condition*(session+trial)+(1+session+trial|subject)+(1+condition+session+trial|stimulus), family="binomial",Data[Data$test==0,],nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))
p=anova(model1trial,model2trial)
p
ModelDiagnostics(model2trial)

model2RT <- glmer(correct~1+condition*(session+trial)*RT+(1+session+trial|subject)+(1+condition+session+trial|stimulus), family="binomial",Data[Data$test==0,],nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))#
p=anova(model1RT,model2RT)
p
ModelDiagnostics(model2RT)

# Post hoc testing
model2individuals <- glmer(correct~ 1 + session + (1 + session|subject) + (1 + session|stimulus), family="binomial",Data[Data$test==0 & Data$condition=='Individuals',],nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))#
ModelDiagnostics(model2individuals)

model2dyads <- glmer(correct~1+session+(1+session|subject)+(1+session|stimulus), family="binomial",Data[Data$test==0 & Data$condition=='Dyads',],nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))#
ModelDiagnostics(model2dyads)

model2individualstrial <- glmer(correct~1+session+trial+(1+session+trial|subject)+(1+session+trial|stimulus), family="binomial",Data[Data$test==0 & Data$condition=='Individuals',],nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))#
ModelDiagnostics(model2individualstrial)

model2dyadstrial <- glmer(correct~1+session+trial+(1+session+trial|subject)+(1+session+trial|stimulus), family="binomial",Data[Data$test==0 & Data$condition=='Dyads',],nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))#
ModelDiagnostics(model2dyadstrial)

model2individualsRT <- glmer(correct~1+(session+trial)*RT+(1+session+trial|subject)+(1+session+trial|stimulus), family="binomial",Data[Data$test==0 & Data$condition=='Individuals',],nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))#
ModelDiagnostics(model2individualsRT)

model2dyadsRT <- glmer(correct~1+(session+trial)*RT+(1+session+trial|subject)+(1+session+trial|stimulus), family="binomial",Data[Data$test==0 & Data$condition=='Dyads',],nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))#
ModelDiagnostics(model2dyadsRT)

```

### Plotting the best model (interaction effects)

```{r TrainingPlot, echo=F, warning=F, message=F}

Data$correct=as.numeric(Data$correct)
Data$correct=Data$correct-1

Data$fit[Data$test==0]=fitted(model2)
sum1 <- summarySEwithin(Data[Data$test==0,], measurevar="correct", withinvars=c("condition","session"), idvar="subject")

p1=ggplot(sum1, aes(x = session, y = correct, color=condition)) + #
        geom_errorbar(aes(ymin=correct-ci, ymax=correct+ci), colour="black", width=.1) +
        geom_line(aes(group=condition)) +
        geom_point(size=3, shape=21, fill="white") +  # 21 is filled circle
        theme_classic()+
        theme(legend.title=element_blank()) + 
        theme(legend.justification=c(1,1), legend.position=c(1,1)) +
        theme(legend.text=element_text(size=14))

p1

Data$correct=as.factor(Data$correct)

```

### Post-hoc tests
Given the interaction effects, we now do post-hoc testing to figure out what generates the interactions.
The mixed effects model corresponds to an ANOVA

```{r TrainingPostHoc, echo=F, warning=F, message=F}

Data$session=as.factor(Data$session)
Data$correct=as.factor(Data$correct)

model3 <- glmer(correct~1+condition*session+(1+session|subject)+(1+condition+session|stimulus), Data[Data$test==0,],family="binomial",nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))

ModelDiagnostics(model3)

Data$session=as.numeric(Data$session)

```

### Bootstrapping estimates

```{r TrainingBootstrap, echo=F, warning=F, message=F}

# First estimating overall pair vs. individual performance
Data$correct=as.numeric(Data$correct)-1
IndTrain = boot(Data$correct[Data$condition=='Individuals' & Data$test==0],statistic=boot.mean,R=10000)

IndCiTrain=boot.ci(IndTrain,type="bca")

DyaTrain = boot(Data$correct[Data$condition=='Dyads' & Data$test==0],statistic=boot.mean,R=10000)
DyaTrain
DyaCiTrain = boot.ci(boot.out=DyaTrain,type="bca")

# Second estimating performance by session

S1Train = boot(Data$correct[Data$session==1 & Data$test==0],statistic=boot.mean,R=10000)
S1TrainCi = boot.ci(boot.out=S1Train,type="bca")

S2Train = boot(Data$correct[Data$session==2 & Data$test==0],statistic=boot.mean,R=10000)
S2TrainCi = boot.ci(boot.out=S2Train,type="bca")

S3Train = boot(Data$correct[Data$session==3 & Data$test==0],statistic=boot.mean,R=10000)
S3TrainCi = boot.ci(boot.out=S3Train,type="bca")

# Finally looking at the third session only
S3Ind = boot(Data$correct[Data$session==3 & Data$condition=='Individuals' & Data$test==0],statistic=boot.mean,R=10000)
S3IndCi = boot.ci(boot.out=S3Ind,type="bca")

# Finally looking at the third session only
S3Dya = boot(Data$correct[Data$session==3 & Data$condition=='Dyads' & Data$test==0],statistic=boot.mean,R=10000)
S3DyaCi = boot.ci(boot.out=S3Dya,type="bca")

```

### Analysis 2 - Test Performance (correctness) as a function of condition (individual vs dyad) and complexity (session)

We use a mixed logistic regression model to assess main effects of condition (pair vs. individual) and complexity (session 1 to 3). We remove individual variability (random intercept plus random slopes due to session) and stimulus variability (random intercepts).

### The first model tests main effects

```{r TestMainEffects, echo=F, warning=F, message=F}

model1 <- glmer(correct~1+condition+session+trial+(1+session+trial|subject)+(1+condition+session+trial|stimulus),Data[Data$test==1,],family="binomial",nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))
ModelDiagnostics(model1)

model1RT <- glmer(correct~1+condition+session+trial+RT+(1+session+trial|subject)+(1+condition+session+trial|stimulus),Data[Data$test==1,],family="binomial",nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))
ModelDiagnostics(model1RT)
```

### The second model tests interaction effects

```{r TestInteractions, echo=F, warning=F, message=F}
Data$correct=as.factor(Data$correct)

#model2 <- glmer(correct ~ condition * (session + trial) + (1 + session + trial|subject) + (1+condition+session+trial|stimulus),Data[Data$test==1,],family="binomial",nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))
model2 <- glmer(correct~condition+(session)+trial+ (1 + session + trial|subject) + (1+condition+session+trial|stimulus),Data[Data$test==1,],family="binomial",nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))
summary(model2)

# Post hoc testing
model2dyads <- glmer(correct~session+trial+(1|subject)+(1|stimulus), family="binomial",Data[Data$test==1 & Data$condition=='Dyads',],nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))#

model2individuals <- glmer(correct~session+trial+(1|subject)+(1|stimulus), family="binomial",Data[Data$test==1 & Data$condition=='Individuals',],nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))#

p=anova(model1,model2)
p

ModelDiagnostics(model2)

model2RT <- glmer(correct~condition*RT*session+trial+(1|subject),Data[Data$test==1,],family="binomial",nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))
summary(model2RT)
#Post hoc testing
model2RTIndividual <- glmer(correct~RT*session+trial+(1+session+trial|subject)+(1+session+trial|stimulus),Data[Data$test==1 & Data$condition=='Individuals',],family="binomial",nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))
model2RTDyad <- glmer(correct~RT*session+trial+(1+session+trial|subject)+(1+session+trial|stimulus),Data[Data$test==1 & Data$condition=='Dyads',],family="binomial",nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))

# 3 way post hoc testing
model2RTIndividual1 <- glmer(correct~RT+trial+(1+trial|subject)+(1+trial|stimulus),Data[Data$test==1 & Data$condition=='Individuals' & Data$session==1,],family="binomial",nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))
model2RTIndividual2 <- glmer(correct~RT+trial+(1+trial|subject)+(1+trial|stimulus),Data[Data$test==1 & Data$condition=='Individuals' & Data$session==2,],family="binomial",nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))
model2RTIndividual3 <- glmer(correct~RT+trial+(1+trial|subject)+(1+trial|stimulus),Data[Data$test==1 & Data$condition=='Individuals' & Data$session==3,],family="binomial",nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))


```

### Plotting the best model (interaction effects)

```{r TestPlot, echo=F, warning=F, message=F}
Data$correct=as.numeric(Data$correct)
Data$correct=Data$correct-1

Data$fit[Data$test==1]=fitted(model2)
sum1 <- summarySEwithin(Data[Data$test==1,], measurevar="correct", withinvars=c("condition","session"), idvar="subject")

p1=ggplot(sum1, aes(x = session, y = correct, color=condition)) + #
        geom_errorbar(aes(ymin=correct-ci, ymax=correct+ci), colour="black", width=.1) +
        geom_line(aes(group=condition)) +
        geom_point(size=3, shape=21, fill="white") +  # 21 is filled circle
        theme_classic()+
        theme(legend.title=element_blank()) + 
        theme(legend.justification=c(1,1), legend.position=c(1,1)) +
        theme(legend.text=element_text(size=14))

p1
Data$correct=as.factor(Data$correct)
```

### Post-hoc testing

Given the interaction effects, we now do post-hoc testing to figure out what generates the interactions.
The mixed effects model corresponds to an ANOVA

```{r TestPostHoc, echo=F, warning=F, message=F}
# Post Hoc testing
Data$session=as.factor(Data$session)
Data$correct=as.factor(Data$correct)

model3 <- glmer(correct~condition*session+(1+session|subject)+(1+session+condition|stimulus), Data[Data$test==1,],family="binomial",nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))

ModelDiagnostics(model3)

model4 <- glmer(correct~session+(1+session|subject)+(1+session|stimulus), Data[Data$test==1 & Data$condition=='Individuals',],family="binomial",nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))

ModelDiagnostics(model4)

model5 <- glmer(correct~session+(1+session|subject)+(1+session|stimulus), Data[Data$test==1 & Data$condition=='Dyads',],family="binomial",nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))

ModelDiagnostics(model5)

Data$session=as.numeric(Data$session)

```

### Bootstrapping estimates

```{r TestBootstrap, echo=F, warning=F, message=F}
Data$correct=as.numeric(Data$correct)-1

IndTest = boot(Data$correct[Data$condition=='Individuals' & Data$test==1],statistic=boot.mean,R=10000)

IndCiTest=boot.ci(IndTest,type="bca")

DyaTest = boot(Data$correct[Data$condition=='Dyads' & Data$test==1],statistic=boot.mean,R=10000)
DyaCiTest = boot.ci(boot.out=DyaTest,type="bca")

S1Test = boot(Data$correct[Data$session==1 & Data$test==1],statistic=boot.mean,R=10000)
S1TestCi = boot.ci(boot.out=S1Test,type="bca")

S2Test = boot(Data$correct[Data$session==2 & Data$test==1],statistic=boot.mean,R=10000)
S2TestCi = boot.ci(boot.out=S2Test,type="bca")

S3Test = boot(Data$correct[Data$session==3 & Data$test==1],statistic=boot.mean,R=10000)
S3TestCi = boot.ci(boot.out=S3Test,type="bca")
```

## Analysis 3

Extracting learning curve from each participant/dyad and predicting:

- condition (do dyads and participants have different learning curves?)

  . also by first cycle only

- post-test performance (does learning curve reflect rule learning?)

### Prepare the data

```{r LearningPreprocess, echo=F, warning=F, message=F}

n=1
condition=""
session=0
subject=""
intercept=0
slope=0
slope2=0
conditionC=""
sessionC=0
subjectC=""
interceptC=0
slopeC=0
slope2C=0
testPerf=0
testPerf2=0
Data$trial2=Data$trial^2
# Create the learning curve dataset
for (k in unique(Data$condition)){
  for (i in unique(Data$subject)) {
    for (l in unique(Data$session)){
      x=Data[Data$test==0 & Data$session==l & Data$subject==i & Data$condition==k,]
      xc=Data[Data$test==0 & Data$session==l & Data$subject==i & Data$condition==k & Data$cycle==1,]
      if (nrow(x)>0) {
        
        model1 <- glmer(correct~trial+(1|stimulus), x,family="binomial",nAGQ=0, control=glmerControl(optimizer = "nloptwrap")) 
        modelC <- glmer(correct~trial+(1|stimulus), xc,family="binomial",nAGQ=0, control=glmerControl(optimizer = "nloptwrap")) 
        
        condition[n]=x$condition[1]
        subject[n]=i
        session[n]=l
        intercept[n] =fixef(model1)[1]
        slope[n] =fixef(model1)[2]
        
        conditionC[n]=xc$condition[1]
        subjectC[n]=i
        sessionC[n]=l
        interceptC[n] =fixef(modelC)[1]
        slopeC[n] =fixef(modelC)[2]
        
        x1=Data[Data$test==1 & Data$session==l & Data$subject==i & Data$condition==k,]
        testPerf[n]=sum(as.numeric(x1$correct))
        testPerf2[n]=testPerf[n]/nrow(x)
        n=n+1
        
      }
    }
  }
}

LearningSet=data.frame(condition,subject,session,intercept,slope,testPerf,testPerf2)
LearningSetC=data.frame(conditionC,subjectC,sessionC,interceptC,slopeC,testPerf,testPerf2)

ggplot(subset(Data,test==1),aes(trial,as.numeric(correct)-1,color=subject))+geom_smooth(se=F)+facet_wrap(condition~session)+guides(color=F)
```

### Predicting condition from learning curve

Main Effects first and Interaction second

```{r LearningPredictingCondition, echo=F, warning=F, message=F}

model2=glmer(condition ~ intercept + slope  + session + (1 + session | subject), LearningSet, family="binomial",nAGQ=0, control=glmerControl(optimizer = "nloptwrap")) 

ModelDiagnostics(model2)

model3=glmer(condition ~ (intercept + slope)  * session + (1 + session | subject), LearningSet, family="binomial",nAGQ=0, control=glmerControl(optimizer = "nloptwrap")) #+ slope2

ModelDiagnostics(model3)
```


### Predicting condition from learning curve in first cycle

```{r LearningFirstCyclePredictingCondition, echo=F, warning=F, message=F}
model2C=glmer(as.factor(condition) ~ interceptC + slopeC  + sessionC + (1 + sessionC | subjectC), LearningSetC, family="binomial", nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))#+ slope2C

ModelDiagnostics(model2C)

model3C=glmer(as.factor(condition) ~ (interceptC + slopeC)  * sessionC + (1 + sessionC | subjectC), LearningSetC, family="binomial", nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))#+ slope2C

ModelDiagnostics(model3C)

```

### Predicting test performance from learning curve

```{r LearningPredictingTest, echo=F, warning=F, message=F}
#model4=glmer(testPerf ~ intercept + slope  + condition + session + (1 + condition + session | subject), LearningSet,family='poisson')

model4=glmer(testPerf ~ intercept + slope + condition + (1 + condition | subject), LearningSet,family='poisson')


ModelDiagnostics(model4)

fit=fitted(model4)

plot4= ggplot(LearningSet,aes(fit,testPerf)) + geom_point()+ stat_smooth(method = "lm")+theme_classic()
plot4

```

### Bootstrapping estimates

```{r LearningBootstrap, echo=F, warning=F, message=F}

IndIntercept = boot(LearningSet$intercept[LearningSet$condition=="1"],statistic=boot.mean,R=10000)
IndInterceptCi = boot.ci(boot.out=IndIntercept,type="bca")

DyaIntercept = boot(LearningSet$intercept[LearningSet$condition=="2"],statistic=boot.mean,R=10000)
DyaInterceptCi = boot.ci(boot.out=DyaIntercept,type="bca")

IndSlope = boot(LearningSet$slope[LearningSet$condition=="1"],statistic=boot.mean,R=10000)
IndSlopeCi = boot.ci(boot.out=IndSlope,type="bca")

DyaSlope = boot(LearningSet$slope[LearningSet$condition=="2"],statistic=boot.mean,R=10000)
DyaSlopeCi = boot.ci(boot.out=DyaSlope,type="bca")

```



### Conversation analysis

Looks at relations between:
- cosine similarity during training and performance during testing
- speech ratio during training and performance during testing

```{r}
# Load the conversation data
DataConv=read.csv('DATA_lemma.csv', sep=",", header=TRUE) # Name of the file
# Calculate performance for pair, session, test
Data$correct=as.numeric(Data$correct)-1
Data$correct=as.factor(Data$correct)

DataConv$Pair=DataConv$Pair+30
DataConv$Cosine[DataConv$Cosine==0]=NA
DataConv$CosinePredict[DataConv$Test==1]=DataConv$Cosine[DataConv$Test==0]
DataConv$CosineDistance[complete.cases(DataConv$Ratio)]=1-DataConv$Cosine[complete.cases(DataConv$Ratio)]
DataConv$CosineDistancePredict[DataConv$Test==1]=DataConv$CosineDistance[DataConv$Test==0]

DataConv=merge(Data,DataConv,by.x=c("subject","session","test"),by.y=c("Pair","Session","Test"),all=T)

CosineModelTrain=glmer(correct~CosineDistance*session+trial+(1+session+trial|subject)+(1+session+trial|stimulus),DataConv[DataConv$test==0,],family=binomial,nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))
summary(CosineModelTrain)

plot5= ggplot(subset(DataConv,test==0),aes(CosineDistance,as.numeric(correct))) + stat_smooth(method = "lm")+theme_classic()
plot5

CosineModelTest=glmer(correct~CosineDistance*session+trial+(1+trial|subject)+(1+session+trial|stimulus),DataConv[DataConv$test==1,],family=binomial,nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))
summary(CosineModelTest)

CosineModelPredict=glmer(correct~CosineDistancePredict+session+trial+(1+session+trial|subject)+(1+session+trial|stimulus),DataConv[DataConv$test==1,],family=binomial,nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))
summary(CosineModelPredict)

p_load(tidyverse)

CosineCleanEstimates=lmer(CosineDistance~1+session+trial+(1+session|subject),DataConv[DataConv$test==0,],REML=F,control=lmerControl(optimizer = "nloptwrap"))

DataConv2=subset(DataConv, test==0 & condition=='Dyads' & complete.cases(DataConv[,c("CosineDistance","session","trial","subject")]))
DataConv2$CosineClean=resid(CosineCleanEstimates)

DataConv2$Accuracy=as.numeric(DataConv2$correct)*100-100
TempData=DataConv2 %>% group_by(subject,session) %>% dplyr::summarise(Performance=mean(Accuracy,na.rm=T),Cosine=mean(CosineClean,na.rm=T))
TempData$Cosine=TempData$Cosine+0.1
  
plot6= ggplot(TempData,aes(Cosine,Performance)) + stat_smooth(method = "lm")+geom_point()+theme_classic()
plot6
```


